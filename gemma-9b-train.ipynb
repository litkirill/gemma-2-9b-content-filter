{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9847801,"sourceType":"datasetVersion","datasetId":6042358},{"sourceId":9887808,"sourceType":"datasetVersion","datasetId":6072252}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U bitsandbytes \"transformers>=4.45.1\" accelerate peft","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:25:37.605132Z","iopub.execute_input":"2024-11-14T14:25:37.605436Z","iopub.status.idle":"2024-11-14T14:26:12.863017Z","shell.execute_reply.started":"2024-11-14T14:25:37.605402Z","shell.execute_reply":"2024-11-14T14:26:12.862079Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: transformers>=4.45.1 in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers>=4.45.1\n  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.1) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.45.1) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.45.1) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.45.1) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.45.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.45.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.45.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.45.1) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, transformers, peft\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed accelerate-1.1.1 bitsandbytes-0.44.1 peft-0.13.2 transformers-4.46.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport copy\n\nimport functools\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\n\nfrom datasets import Dataset\nfrom transformers import (\n    BitsAndBytesConfig,\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    Gemma2Config,\n    PreTrainedTokenizerBase, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\n\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom huggingface_hub import HfFolder\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, log_loss","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:12.864750Z","iopub.execute_input":"2024-11-14T14:26:12.865064Z","iopub.status.idle":"2024-11-14T14:26:43.520645Z","shell.execute_reply.started":"2024-11-14T14:26:12.865031Z","shell.execute_reply":"2024-11-14T14:26:43.519873Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:43.521859Z","iopub.execute_input":"2024-11-14T14:26:43.522649Z","iopub.status.idle":"2024-11-14T14:26:43.639578Z","shell.execute_reply.started":"2024-11-14T14:26:43.522600Z","shell.execute_reply":"2024-11-14T14:26:43.638635Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\n  token=hf_token,\n  add_to_git_credential=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:43.641893Z","iopub.execute_input":"2024-11-14T14:26:43.642281Z","iopub.status.idle":"2024-11-14T14:26:43.767314Z","shell.execute_reply.started":"2024-11-14T14:26:43.642242Z","shell.execute_reply":"2024-11-14T14:26:43.766235Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"@dataclass\nclass Config:\n    output_dir: str = \"LitvinKA/gemma-2-9b-content-filter\"\n    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n    optim_type: str = \"adamw_8bit\"\n    per_device_train_batch_size: int = 4\n    gradient_accumulation_steps: int = 8\n    per_device_eval_batch_size: int = 8\n    n_epochs: int = 1\n    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n    lr: float = 2e-4\n    warmup_ratio: int = 0.05\n    lora_r: int = 16\n    lora_alpha: float = lora_r * 2\n    lora_dropout: float = 0.05\n    lora_bias: str = \"none\"\n    train_size: int = 150000\n    val_size: int = 10000\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:43.768587Z","iopub.execute_input":"2024-11-14T14:26:43.768989Z","iopub.status.idle":"2024-11-14T14:26:43.776989Z","shell.execute_reply.started":"2024-11-14T14:26:43.768942Z","shell.execute_reply":"2024-11-14T14:26:43.776073Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"LitvinKA/gemma-2-9b-content-filter\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    optim=config.optim_type,\n    fp16=True,\n    learning_rate=config.lr,\n    warmup_ratio=config.warmup_ratio,  \n    # logging & evaluation strategies\n    logging_dir=f\"{config.output_dir}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=800,\n    eval_strategy=\"steps\",\n    eval_steps=800,\n    save_strategy=\"steps\",\n    save_steps=800,\n    # push to hub parameters\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_strategy=\"every_save\",\n    hub_model_id=config.output_dir,\n    hub_token=HfFolder.get_token(),\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:43.778200Z","iopub.execute_input":"2024-11-14T14:26:43.778502Z","iopub.status.idle":"2024-11-14T14:26:43.903805Z","shell.execute_reply.started":"2024-11-14T14:26:43.778472Z","shell.execute_reply":"2024-11-14T14:26:43.902877Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:43.904943Z","iopub.execute_input":"2024-11-14T14:26:43.905284Z","iopub.status.idle":"2024-11-14T14:26:43.911451Z","shell.execute_reply.started":"2024-11-14T14:26:43.905249Z","shell.execute_reply":"2024-11-14T14:26:43.910466Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = Gemma2ForSequenceClassification.from_pretrained(\n    config.checkpoint,\n    num_labels=1,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:26:43.912728Z","iopub.execute_input":"2024-11-14T14:26:43.913051Z","iopub.status.idle":"2024-11-14T14:29:16.324908Z","shell.execute_reply.started":"2024-11-14T14:26:43.913015Z","shell.execute_reply":"2024-11-14T14:29:16.323887Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bfe9c8fad3448c8bdc3dc54956a7021"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f601649989e44dc2a06cd0099b7c14cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-9b-it-bnb-4bit and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): Gemma2ForSequenceClassification(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n        (layers): ModuleList(\n          (0-15): 16 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n              (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n              (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n          )\n          (16-41): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=1, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=1, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:16.326336Z","iopub.execute_input":"2024-11-14T14:29:16.326752Z","iopub.status.idle":"2024-11-14T14:29:16.342311Z","shell.execute_reply.started":"2024-11-14T14:29:16.326707Z","shell.execute_reply":"2024-11-14T14:29:16.341239Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"trainable params: 30,248,448 || all params: 9,271,958,016 || trainable%: 0.3262\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/d/tatianamerzl/wb-winter-24/train.csv')\ntrain_data.columns = ['ID', 'text', 'labels']","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:16.346868Z","iopub.execute_input":"2024-11-14T14:29:16.347188Z","iopub.status.idle":"2024-11-14T14:29:17.805481Z","shell.execute_reply.started":"2024-11-14T14:29:16.347129Z","shell.execute_reply":"2024-11-14T14:29:17.804680Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"num_positive = train_data['labels'].sum()  \nnum_negative = len(train_data) - num_positive \n\npos_weight = num_negative / num_positive\n\nlabel_weights = torch.tensor([pos_weight], dtype=torch.float32, device=model.device)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:17.806583Z","iopub.execute_input":"2024-11-14T14:29:17.806890Z","iopub.status.idle":"2024-11-14T14:29:17.819782Z","shell.execute_reply.started":"2024-11-14T14:29:17.806858Z","shell.execute_reply":"2024-11-14T14:29:17.818996Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nval_split_ratio = 0.10\n\ntrain_data, validation_data = train_test_split(train_data, test_size=val_split_ratio, random_state=42, stratify=train_data['labels'].values, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:17.821045Z","iopub.execute_input":"2024-11-14T14:29:17.821431Z","iopub.status.idle":"2024-11-14T14:29:17.981358Z","shell.execute_reply.started":"2024-11-14T14:29:17.821390Z","shell.execute_reply":"2024-11-14T14:29:17.980253Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_data[:config.train_size])\nvalidation_dataset = Dataset.from_pandas(validation_data[:config.val_size])","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:17.982730Z","iopub.execute_input":"2024-11-14T14:29:17.983058Z","iopub.status.idle":"2024-11-14T14:29:18.361685Z","shell.execute_reply.started":"2024-11-14T14:29:17.983024Z","shell.execute_reply":"2024-11-14T14:29:18.360858Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:18.362706Z","iopub.execute_input":"2024-11-14T14:29:18.362979Z","iopub.status.idle":"2024-11-14T14:29:20.706047Z","shell.execute_reply.started":"2024-11-14T14:29:18.362949Z","shell.execute_reply":"2024-11-14T14:29:20.704904Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db2c53db8e7645d0bfbe05da59bd0faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8eeead5e7d4b35a0278ef37da2e865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687b941dde2a4225bfe48a304a9226a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d04c235d0ae42edafb6f905b7f04bd3"}},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize helper function\ndef tokenize(batch):\n    tokenized_inputs = tokenizer(batch['text'])\n    tokenized_inputs['labels'] = batch['labels']\n    return tokenized_inputs\n\n# Tokenize dataset\ntokenized_train_data = train_dataset.map(tokenize, batched=True)\ntokenized_validation_data = validation_dataset.map(tokenize, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:20.707449Z","iopub.execute_input":"2024-11-14T14:29:20.707774Z","iopub.status.idle":"2024-11-14T14:29:30.713876Z","shell.execute_reply.started":"2024-11-14T14:29:20.707739Z","shell.execute_reply":"2024-11-14T14:29:30.712980Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc5d387e7de4796ba42567dd84bed96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a078173db7c478d8594fbf9b4cde97b"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    logits, labels = eval_preds.predictions, eval_preds.label_ids\n    \n    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n    \n    pred_classes = (probs >= 0.5).astype(int)\n    \n    acc = accuracy_score(labels, pred_classes)\n    precision = precision_score(labels, pred_classes)\n    recall = recall_score(labels, pred_classes)\n    f1 = f1_score(labels, pred_classes)\n    loss = log_loss(labels, probs)\n    \n    return {\n        \"accuracy\": acc,\n        \"log_loss\": loss,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:30.715308Z","iopub.execute_input":"2024-11-14T14:29:30.715965Z","iopub.status.idle":"2024-11-14T14:29:30.724145Z","shell.execute_reply.started":"2024-11-14T14:29:30.715918Z","shell.execute_reply":"2024-11-14T14:29:30.723128Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def __init__(self, label_weights, **kwargs):\n        super().__init__(**kwargs)\n        self.label_weights = label_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        labels = labels.unsqueeze(1)\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        \n\n        loss = F.binary_cross_entropy_with_logits(\n            logits, labels.to(torch.float32), pos_weight=self.label_weights\n        )\n        \n        return (loss, outputs) if return_outputs else loss\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:30.725618Z","iopub.execute_input":"2024-11-14T14:29:30.726339Z","iopub.status.idle":"2024-11-14T14:29:30.738358Z","shell.execute_reply.started":"2024-11-14T14:29:30.726293Z","shell.execute_reply":"2024-11-14T14:29:30.737601Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch, tokenizer):\n    dict_keys = ['input_ids', 'attention_mask', 'labels']\n    d = {k: [torch.tensor(dic[k]) for dic in batch] for k in dict_keys}\n    d['input_ids'] = pad_sequence(\n        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n    )\n    d['attention_mask'] = pad_sequence(\n        d['attention_mask'], batch_first=True, padding_value=0\n    )\n    d['labels'] = torch.stack(d['labels'])\n    return d","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:30.739521Z","iopub.execute_input":"2024-11-14T14:29:30.739831Z","iopub.status.idle":"2024-11-14T14:29:30.755960Z","shell.execute_reply.started":"2024-11-14T14:29:30.739798Z","shell.execute_reply":"2024-11-14T14:29:30.755031Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_train_data,\n    eval_dataset = tokenized_validation_data,\n    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n    compute_metrics = compute_metrics,\n    label_weights = label_weights,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:30.757265Z","iopub.execute_input":"2024-11-14T14:29:30.758145Z","iopub.status.idle":"2024-11-14T14:29:31.501760Z","shell.execute_reply.started":"2024-11-14T14:29:30.758100Z","shell.execute_reply":"2024-11-14T14:29:31.500932Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:29:31.502916Z","iopub.execute_input":"2024-11-14T14:29:31.503226Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1794' max='4687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1794/4687 11:55:34 < 19:15:13, 0.04 it/s, Epoch 0.38/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Log Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>800</td>\n      <td>0.513000</td>\n      <td>0.170869</td>\n      <td>0.983700</td>\n      <td>0.057399</td>\n      <td>0.908063</td>\n      <td>0.967095</td>\n      <td>0.936650</td>\n      <td>1796.567300</td>\n      <td>5.566000</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.271000</td>\n      <td>0.230721</td>\n      <td>0.984400</td>\n      <td>0.095538</td>\n      <td>0.913505</td>\n      <td>0.966292</td>\n      <td>0.939158</td>\n      <td>1796.077200</td>\n      <td>5.568000</td>\n      <td>0.696000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"}]}]}